\documentclass[bj, preprint]{imsart}
\RequirePackage[OT1]{fontenc}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amsthm,amsmath,natbib,booktabs,cleveref}
\newcommand{\cexp}[1]{\left<#1\right>}
\usepackage{bbm}
\newcommand{\onebb}{\mathbbm{1}}
\newcommand{\Ebb}{\mathbb{E}}

% put your definitions there:
\endlocaldefs
\input{../utils/preamble}


\begin{document}

\begin{frontmatter}

\title{{\Large STATS 507 Project Report:} \\ 
\bf \texttt{MovieLens} Datasets---Predicting and Analyzing User Ratings of Movies}
\runtitle{\texttt{MovieLens}---Predicting and Analyzing Movie Ratings}

\begin{aug}
\author{
\fnms{Trong Dat} 
\snm{Do}
\thanksref{a,e1}
\ead[label=e1,mark]{dodat@umich.edu}
}
\and
\author{
\fnms{Simon} 
\snm{Fontaine}
\thanksref{a,e2}
\ead[label=e2,mark]{simfont@umich.edu}
}
\address[a]{University of Michigan, Department of Statistics. West Hall, 1085 South University, Ann Arbor, MI, U.S.A., 48109. \printead{e1,e2}}
\runauthor{Trong Dat Do and Simon Fontaine}
\affiliation{University of Michigan, Department of Statistics}
\end{aug}



%\begin{abstract}

%\end{abstract}

\tableofcontents
\listoftodos
\end{frontmatter}



%------------------------------------------------------------------------------
\section{The \texttt{MovieLens} Datasets}\label{sec:dataset}

The \texttt{MovieLens} datasets \citep{harper2015MovieLensDatasetsHistory} contains user ratings of a variety of movies continuously collected starting from 1998. 
In addition to the \texttt{user}-\texttt{movie}-\texttt{rating} pairings, the datasets contains information about movie genres, word tagging of movies provided by users and user demographic information. 

We will consider the \texttt{MovieLens 100K Dataset}\footnote{Available at \url{https://grouplens.org/datasets/movielens/100k/}}, which is one of the multiple datasets provided by \texttt{GroupLens}\footnote{Organization website: \url{https://grouplens.org/}}. 
We will be interested in this particular dataset because it contains additional demographic information about the users in the dataset. 
To include tagging information, we also consider the \texttt{MovieLens Tag Genome Dataset}\footnote{Available at \url{https://grouplens.org/datasets/movielens/tag-genome/}}. 
Here is a summary of the contents of the datasets that will be used\footnote{From the \texttt{README.txt} file attached to the datasets (\url{http://files.grouplens.org/datasets/movielens/ml-100k-README.txt}, \url{http://files.grouplens.org/datasets/tag-genome/README.html)}}:

\begin{description}
	\item[\texttt{MovieLens 100K Dataset}] 
	The dataset was collected from the \texttt{MovieLens} website (\url{movielens.umn.edu}) between September 19th, 1997 through April 22nd, 1998. 
	It has been pre-processed and cleaned to include only examples where the users have made at least 20 ratings during the collection period and where demographic information are complete. 
	In the \texttt{u.data} file, there are \num{100000} ratings on the scale of 1 to 5, taking only integer values. 
	It contains the following entries: \texttt{user id}, \texttt{item id}, \texttt{rating}, \texttt{timestamp}. 
	In the \texttt{u.item} file, there are \num{1681} movies with the following information: \texttt{movie id}, \texttt{movie title}, \texttt{release date}, \texttt{IMDb URL} and 19 columns indicating movie genre with 0-1 encoding where 1 denotes that the movie is of the corresponding genre. 
	In the \texttt{u.user} file, there are \num{943} users with the following information: \texttt{user id}, \texttt{age}, \texttt{gender}, \texttt{occupation} (see \texttt{u.occupation} file for details) and \texttt{zip code}.
	\item[\texttt{MovieLens Tag Genome Dataset}] 
	This dataset contains tagging information of \num{9734} movies and \num{1128} tags. 
	In particular, the \texttt{tag\_relevance} file contains the relevance of all tags for all movies reported on a continuous scale from 0 to 1, where 1 indicates strong relevance.
\end{description}


%------------------------------------------------------------------------------
\section{Research Questions}\label{sec:questions}

\subsection{Prediction Modeling}

Our first research question is to construct a predictive model for the user ratings using the available information. 
In particular, we wish to produce a model that is able to accurately predict the movie rating (for some movie already in the dataset) by a given user (also in the dataset). 
This model could then be part of a \textit{recommendation system} where the predicted rating could be used as input to produce the recommendations.

\subsection{Exploratory Analysis}

A secondary research question we are interested in is to analyze the effect of the available information on the user ratings. 
For example, we could look for genres and tags that are related to movies with better ratings. 
Then, we can perform more granular analyses using the demographic data: this could allow to extract correlations between population groups and movie interests. 
The insights recovered from such analyses could be relevant for decision-making such as identifying which movies to produce and which population groups to target with advertisement.

%------------------------------------------------------------------------------
\section{Methodology}\label{sec:method}

%------------------------------------------------------------------------------
\subsection{Data Pre-processing}\label{subsec:method.preprocess}

The \texttt{MovieLens 100K Dataset} and the \texttt{MovieLens Tag Genome Dataset} have both been extensively cleaned by \texttt{GroupLens}\footnote{See the two \texttt{README} files for details: \url{http://files.grouplens.org/datasets/movielens/ml-100k-README.txt} and \url{http://files.grouplens.org/datasets/tag-genome/README.html}}. 
Thus, the main pre-processing we have to perform is the merging of the different datasets and the creation of the training and testing sets.

%------------------------------------------------------------------------------
\subsubsection{Merging the Datasets}\label{subsubsec:method.preprocess.merge}

First, the three datasets in \texttt{MovieLens 100K Dataset}, corresponding to user data, movie data and the ratings, have unique IDs which allows us to easily match them. 
There were no ratings which we were unable to match to users and/or movies in these datasets.

Second, the \texttt{MovieLens Tag Genome Dataset} also has unique IDs identifying movies, but they differ from those in the \texttt{MovieLens 100K Dataset}. 
Hence, we resort to matching the movie on the movie name. 
Direct matching of the strings allowed of to match a large proportion ($\sim$80\%) of the movies. 
There were also some movie that we were able to match using simple rules. 
For example, the movie names include the year of publication which were often mismatched by a year and prevented direct matching. 
Also, other movie names included the original name (in a foreign language) in either of the two datasets which prevented direct matching, but could still be detected. 
Finally, visual inspection (mostly by hand) of the remaining unmatched movies allowed to identify a few additional matches.
In total, we were able to match \num{1558} out of the \num{1681} in the \texttt{MovieLens 100K Dataset}. 
We therefore dropped all ratings of the movies which we could not matched. Fortunately, only very marginal movies did not make the cut and only \num{588} ratings out of \num{100000} had to be dropped: the resulting dataset therefore is composed of \num{99412} ratings on \num{1558} movies by \num{943} users. 

%------------------------------------------------------------------------------
\subsubsection{Data subsetting}\label{subsubsec:method.preprocess.subset}

Upon joining the different datasets, we further subset the data in order to insure adequate representation of all included movies. 
In particular, we identify that some movies have small frequency (some appearing only once, for example). 
To fix this problem, we omit all ratings of movies which appear less than 20 times in the dataset. 
The final dataset then contains \num{94692} ratings on \num{935} movies; all \num{943} users remain in the dataset.

%------------------------------------------------------------------------------
\subsubsection{Data Splitting}\label{subsubsec:method.preprocess.split}

In order to assess the performance of the various models we consider, we proceed to the usual training-testing splitting. 
The training set consists of 75\% of the dataset (\num{71035} ratings) and the testing set of 25\% of the data (\num{23657} ratings). 
For consistency of results, the splitting was kept constant throughout the analysis.
To insure adequate representation of all movies in each sets, we proceeded to a stratified sampling of ratings within movies.

Furthermore, the training set was split into cross-validation sets. 
In particular, the \num{71035} ratings were each assigned to one of 5 folds randomly, yielding training sets of size \num{56828}, on average, and validations sets of size \num{14207}, on average. 
Again, this splitting was kept constant throughout the analysis.

%------------------------------------------------------------------------------
\subsection{Modeling}\label{subsec:method.models}

Describe models, what are the tuning parameters, etc.

%------------------------------------------------------------------------------
\subsubsection{K-Nearest-Neighbors}\label{subsubsec:method.models.knn}

%------------------------------------------------------------------------------
\paragraph{Implementation details.}\label{par:method.models.knn.impl}

%------------------------------------------------------------------------------
\subsubsection{Neural Networks}\label{subsubsec:method.models.nn}

%------------------------------------------------------------------------------
\paragraph{Implementation details.}\label{par:method.models.nn.impl}

%------------------------------------------------------------------------------
\subsubsection{Matrix completion}\label{subsubsec:method.models.svd}
Building a recommendation system by doing matrix completion is one of "collaborative filtering" techniques. This means we will try to suggest movies to an user by inspecting the ratings of "similar" users. To be more specific, it is assumed that the taste, or the ratings, made by users follows some low-rank structure, and use that assumption to make prediction. 

Let the hyper parameters embedded dimension be $k$ and the iteration $iter\_nums$ We train the test set as follows

1. Create a pivot matrix $A$ in which rows are users and columns are movies. The (i,j)-cell of the matrix will be the visible rating of user i for movie j, NA if there is no information about that in the train set. We also create a masked matrix which indicate which cell in pivot matrix is NA. Fill the NA cells in $A$ with '0'.

2. For each t in range($iter\_num$): 
\begin{enumerate}
	\item Fill all the non-masked cell in $A$ with the original visible rating.
	\item Approximate $A$ by its truncated SVD. 
\end{enumerate}
By iterating two steps above, it allows us to reuse the visible information over and over, and extract the low rank structure of ratings. It is proved that if the underlying structure is indeed low-rank, and the masked matrix is not too dense, then this procedure can reconstruct the original matrix. (add details)

%------------------------------------------------------------------------------
\paragraph{Implementation details.}\label{par:method.models.svd.impl}

%------------------------------------------------------------------------------
\subsubsection{Restricted Boltzmann Machine}\label{subsubsec:method.models.rbm}
Similar to matrix completion, Restricted Boltzmann Machine (RBM) is also a collaborative filtering method. It assumes ratings of users are affected by some latent binary variables. So that we construct a probabilistic model where there is $F$ hidden nodes in total, and a bipartite graph for each user, connecting the hidden variables with the visible ratings. 

%------------------------------------------------------------------------------
\paragraph{Model description.}\label{par:method.models.rbm.model}


Denote hidden variables by $\textbf{h} = (h_1, h_2,\dots, h_F)$ and suppose there are $K$ rating scores in total ($5$ for our data set). For now, to reduce the burden of notation, we only consider the model for each user and will talk about the connection between them later. For an user, suppose he rated $m$ movie $1, \dots, m$, and we write the visible ratings $\textbf{V} = (v_i^k)_{i,k}$ where $v_i^k = 1$ if he rates $k$ for movie $i$, and $0$ otherwise. The generative model is defined by
\begin{equation}\label{GenModelRBM}
p(\textbf{V}, \textbf{h}) = \dfrac{1}{Z} \exp(-E(\textbf{V}, \textbf{h})),
\end{equation}
where $Z = \sum_{\textbf{V}', \textbf{h}'}$ is the normalization. The term $E(V, h)$ is so called Energy and defined by
\begin{equation}\label{EnergyRBM}
E(V,h) = - \sum_{i=1}^{m} \sum_{j=1}^{F} \sum_{k=1}^{K} W_{ij}^k h_j v_i^k - \sum_{i=1}^{m} \sum_{k=1}^{K} b_i^k v_i^k - \sum_{j=1}^{F} h_j b_j,
\end{equation}
where $W_{ij}^k$ is the weight connect from $h_j$ to $v_i^k$, $b_i^k$ is bias for $v_i^k$ and $b_j$ is the bias for $h_j$. Therefore, each user has different bipartite graph model and visible ratings $\textbf{V}$, but they all share the same weight $(W_{ij}^k)$ and bias $(b_i^k), (b_j)$.   

From this, we can calculate the conditional probability 
\begin{equation}\label{pv|h}
p(v_i^k = 1| \textbf{h}) = \dfrac{\exp(b_{i}^{k} + \sum_{j=1}^{F} h_j W_{ij}^k)}{\sum_{l=1}^{k} \exp(b_{i}^{l} + \sum_{j=1}^{F} h_j W_{ij}^l)}, 
\end{equation}
and
\begin{equation}\label{ph|v}
p(h_j = 1 | \textbf{V}) = \sigma(b_j + \sum_{i=1}^{m}\sum_{k=1}^{K} v_i^k W_{ij}^{k}),
\end{equation}
where $\sigma$ is the sigmoid function. The marginal distribution for $\textbf{V}$ is
$$p(\textbf{V}) = \dfrac{1}{Z} \sum_{\textbf{h'}} \exp(-E(\textbf{V}, \textbf{h'})).$$
Our aim is to find $(W_{ij}^{k}, b_{i}^{k}, b_j)_{i,j,k}$ maximize the marginal distribution for visible ratings $\textbf{V}$, which is known as Maximum Likelihood method. We will employ the gradient descent to find the minimizer. 

%------------------------------------------------------------------------------
\paragraph{Learning.}\label{par:method.models.rbm.learning}

We need to compute the gradient of $\log p(\textbf{V})$ in order to perform gradient ascent. By chain's rule
\begin{align*}
\dfrac{\partial \log p(\textbf{V})}{\partial W_{ij}^{k}} &= \dfrac{1}{p(\textbf{V})} \dfrac{\partial p(\textbf{V})}{\partial W_{ij}^{k}} \\
& = \dfrac{1}{p(\textbf{V})} \left(\dfrac{\sum_{\textbf{h}} \onebb_{[h_j = 1, v_{i}^{k} = 1]} p(\textbf{V}, \textbf{h})}{Z} - \dfrac{p(\textbf{V}) \sum_{\boldsymbol{\nu}, \textbf{h}}\onebb_{[h_j = 1, \boldsymbol{\nu}_{i}^{k} = 1]} p(\boldsymbol{\nu}, \textbf{h})}{Z} \right)\\
& = \Ebb_{h|\textbf{V}}(v_i^k h_j) - \Ebb(\nu_i^k h_j)\\
& =: \cexp{v_i^k h_j}_{data} - \cexp{\nu_i^k h_j}_{model}
\end{align*}
where $\cexp{\cdot}_{subscript}$ can be understand as conditional expectation with respect to the subscript. 
Similarly, we have
\begin{equation}
\dfrac{\partial p(\textbf{V})}{\partial b_j}  = \Ebb_{h|\textbf{V}} (h_j) - \Ebb(h_j) =  \cexp{h_j}_{data} - \cexp{h_j}_{model},
\end{equation}
and 
\begin{equation}
\dfrac{\partial p(\textbf{V})}{\partial b_i^k}  = v_i^k - \Ebb(\nu_i^k) = v_i^k - \cexp{\nu_i^k}_{model}.
\end{equation}
In each equation, it is well-known to call the first term "positive statistics" and second term "negative statistics". So we have the analytical representation of gradient of weights and biased terms. In every equation, the first term is easy to compute thanks to \eqref{ph|v}. However, computing the terms $\cexp{\nu_{i}^{k}}_{model}, \cexp{h_j}_{model}$, and $\cexp{\nu_{i}^{k}}_{model}$ requires to take the sum over all value of $\boldsymbol{\nu}, \textbf{h}$, which takes exponential time and makes the algorithm inefficiently. We will instead use the Contrastive Divergence (CD) (Hinton, 2002): Approximating the $\cexp{\nu_i^k h_j}_{model}$ with $\cexp{\nu_i^k h_j}_{recon}$, which  is the approximated construction of $\cexp{\nu_i^k h_j}_{model}$ by using an idea similar to Gibbs' sampler.

%------------------------------------------------------------------------------
\paragraph{Computing gradient using Constrastive Divergence.}\label{par:method.models.rbm.grad}

The idea behind Contrastive Divergence (Hilton 2002) is to approximate gradient by difference of two Kullback-Leibler divergences, but ignores one trickly term (add details). Although this is a crude approximation, it turns out to work really well in many applications. The Constrastive Divergence with $n$ steps ($CD_n$) can be interpreted as follows. Given the visible ratings $\textbf{V}$, first we sample binary value for hidden units $h_{data} = h$ by equation \eqref{ph|v}. Then we do $n$ steps of Gibbs sampling, each contains two smaller steps
\begin{enumerate}
	\item Sample $\boldsymbol{\nu} \leftarrow \boldsymbol{h}$ based on equation \eqref{pv|h}.
	\item Sample $\boldsymbol{h} \leftarrow \boldsymbol{\nu}$ based on equation \eqref{ph|v}.
\end{enumerate}
To derive the positive statistics, we use the value of data $\textbf{V}$ and $h_{data}$. Although we can calculate it analytically using \eqref{ph|v}, but using the sample values can reduce the noise when we take the difference with the negative statistics. Hence we have
$$\cexp{v_i^k h_j}_{data} \leftarrow v_i^k h_{data, j}, \quad \cexp{h_j}_{data} \leftarrow h_{data, j}$$
When collect the negative statistics, it is advised in Hilton that in the last step, we should only collect $\boldsymbol{h}$ as the probability $p(\textbf{h}|\boldsymbol{\nu})$ (but not sample from it), and have     
$$\cexp{\nu_i^k h_j}_{recon} \leftarrow \nu_i^k h_{j},\quad \cexp{h_j}_{recon} \leftarrow h_{data, j},\quad \cexp{\nu_i^k}_{recon} \leftarrow \nu_i^k.$$

%------------------------------------------------------------------------------
\paragraph{Making predictions.}\label{par:method.models.rbm.pred}

Knowing a set of visible unit $\textbf{V}$, we can predict rating for a new movie by using Bayes' rule and marginalize $\textbf{h}$
\begin{align*}
p(v_q^{k} =  1 |\textbf{V}) & \propto p(v_q^{k} = 1, \textbf{V})  \\
&  \propto \sum_{\textbf{h}} p(v_q^{k} = 1, \textbf{V}, \textbf{h})\\
& \propto \exp(b_k^q) \prod_{j=1}^{q} \sum_{h_j\in \{0,1\}} \exp(\sum_{il} v_i^{l} h_j W_{ij}^{l} + h_j W_{qj}^{k} + h_jb_j)\\
& = \exp(b_k^q) \prod_{j=1}^{q} \left( 1 +  \exp(\sum_{il} v_i^{l} W_{ij}^{l} +  W_{qj}^{k} + b_j)\right)
\end{align*}


%------------------------------------------------------------------------------
\paragraph{Implementation details.}\label{par:method.models.rbm.impl}





%------------------------------------------------------------------------------
\section{Preliminary and Final report}\label{sec:report}
In our Preliminary report, we wish to address the first question: that is, applying each approach to the \texttt{MovieLens} dataset and compare them. We will present the advantages and disadvantages of each approach and interpret the result. In the Final report, it is expected to have a comprehensible answer for both questions. One more potential question we will address if time allows is to combine the approaches above to derive the best recommendetion algorithm. 

%------------------------------------------------------------------------------
\bibliographystyle{imsart-nameyear}
\bibliography{../utils/references}{}





\end{document}
